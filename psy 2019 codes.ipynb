{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import plot_confusion_matrix, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC \n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from textstat import syllable_count, flesch_reading_ease, dale_chall_readability_score\n",
    "from pymagnitude import *\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc(text):\n",
    "    tokens = WordPunctTokenizer().tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [re.sub('[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    tk = ' '.join(tokens) \n",
    "    tk = nlp(tk)\n",
    "    tk = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in tk])\n",
    "    return tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "def plot_cm(classifier, X_test, y_test):\n",
    "    class_names = np.unique(y_test)\n",
    "    titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                      (\"Normalized confusion matrix\", 'true')]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     display_labels=class_names,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "cdmp = pd.read_csv(r'psy 2018 cdmp r1.csv')\n",
    "cdmp.CDMP_R1 = cdmp.CDMP_R1.astype('int64')\n",
    "cdmp['text'] = cdmp['cdmptext'].apply(pre_proc)\n",
    "xtrain_cdmp, xtest_cdmp, ytrain_cdmp, ytest_cdmp = train_test_split(cdmp.text,cdmp.CDMP_R1, \n",
    "                                                                    test_size=0.3, random_state=1008)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_pp= imbpipeline([('tfvect',TfidfVectorizer()),\n",
    "                     ('oversamp',RandomOverSampler(random_state=10023)),\n",
    "                     ('svm',SVC(random_state=19039))])\n",
    "svm1_pp_param= {'tfvect__stop_words': ['english',None],\n",
    "               'tfvect__ngram_range':[(1,1),(1,2),(2,2),(1,3),(2,3),(3,3)],\n",
    "               'tfvect__max_features': [500,700, 1000, 2000, 3000, 4000, 5000, 6000],\n",
    "               'svm__kernel': ['linear','poly','rbf','sigmoid'],\n",
    "               'svm__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'svm__tol':[0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "svm1_gs = GridSearchCV(estimator = svm1_pp,\n",
    "                      param_grid = svm1_pp_param,\n",
    "                      scoring = make_scorer(kappa),\n",
    "                      n_jobs=-1,\n",
    "                     cv = 5)\n",
    "#cdmp\n",
    "svm1_gs = svm1_gs.fit(xtrain_cdmp,ytrain_cdmp)\n",
    "print('CDMP: ', svm1_gs.best_score_)\n",
    "print('CDMP: ', svm1_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm1_model = imbpipeline([('tfvect',TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)),\n",
    "                         ('oversamp',RandomOverSampler(random_state=10023)),\n",
    "                     ('svm',SVC(C=10, kernel='rbf', tol=1, random_state=19039))])\n",
    "svm1_model.fit(xtrain_cdmp,ytrain_cdmp)\n",
    "svm1_pred = svm1_model.predict(xtest_cdmp)\n",
    "svm1_kappa = kappa(svm1_pred,ytest_cdmp)\n",
    "print('QWK for cdmp (SVM1):', round(svm1_kappa,4))\n",
    "plot_cm(svm1_model,xtest_cdmp,ytest_cdmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GloVe pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-vectors\n",
    "embeddings_index = {}\n",
    "\n",
    "\n",
    "\n",
    "f = open(r'glove.6B.200d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:],dtype='float32')\n",
    "f.close()\n",
    "\n",
    "# create a tokenizer\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(cdmp['text'])\n",
    "word_index = token.word_index \n",
    "\n",
    "# max length of text\n",
    "max_len = max([len(s.split()) for s in xtrain_cdmp])\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors\n",
    "tr_seq_x_cdmp = sequence.pad_sequences(token.texts_to_sequences(xtrain_cdmp), maxlen=max_len,padding='post')\n",
    "te_seq_x_cdmp = sequence.pad_sequences(token.texts_to_sequences(xtest_cdmp), maxlen=max_len, padding='post')\n",
    "\n",
    "#create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index)+1,200))\n",
    "for w, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(w)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text to vector\n",
    "\n",
    "def txt2vec(text):\n",
    "    M = []\n",
    "    for w in WordPunctTokenizer().tokenize(text):\n",
    "        if not w.isalpha():\n",
    "            continue\n",
    "        if w in embeddings_index:\n",
    "            M.append(embeddings_index[w])\n",
    "    M = np.array(M)\n",
    "    v = M.mean(axis=0)\n",
    "    if type(v)!= np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pt_x_cdmp = np.array([txt2vec(s) for s in xtrain_cdmp])\n",
    "te_pt_x_cdmp = np.array([txt2vec(s) for s in xtest_cdmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_pt = imbpipeline([('oversamp', RandomOverSampler(random_state=10023)),\n",
    "                      ('svm', SVC(random_state=19039))])\n",
    "\n",
    "svm1_pt_param= {'svm__kernel': ['linear','poly','rbf','sigmoid'],\n",
    "               'svm__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'svm__tol':[0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "svm1_pt_gs = GridSearchCV(estimator = svm1_pt,\n",
    "                      param_grid = svm1_pt_param,\n",
    "                      scoring = make_scorer(kappa),\n",
    "                      n_jobs=-1,\n",
    "                     cv = 5)\n",
    "#cdmp\n",
    "svm1_pt_gs = svm1_pt_gs.fit(tr_pt_x_cdmp,ytrain_cdmp)\n",
    "print('CDMP (svm pretrained): ', svm1_pt_gs.best_score_)\n",
    "print('CDMP (svm pretrained): ', svm1_pt_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_pt_pred = svm1_pt_gs.predict(te_pt_x_cdmp)\n",
    "svm1_pt_kappa = kappa(svm1_pt_pred,ytest_cdmp)\n",
    "print('QWK for cdmp (svm1 pretrained): ', round(svm1_pt_kappa,4))\n",
    "plot_cm(svm1_pt_gs, te_pt_x_cdmp,ytest_cdmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_word(x,y,K):\n",
    "    x['num_word'] = x['text'].apply(lambda x: len(x.split()))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['num_word'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of number of words ' + y[:-3])\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = num_of_word(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_len(x,y,K):\n",
    "    x['mean_w_len'] = x['text'].apply(lambda x: sum(len(w) for w in x.split())/len(x.split()))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['mean_w_len'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of mean word length ' + y[:-3])\n",
    "    plt.xlabel('Mean Word Length')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = mean_word_len(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_r(x,y,K):\n",
    "    stop_words = stopwords.words('english')\n",
    "    x['stopword_r'] = x['text'].apply(lambda x: len([w for w in x.split() if w in stop_words])/len(x.split()))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['stopword_r'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of stopword ratio ' + y[:-3])\n",
    "    plt.xlabel('Stopword ratio')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = stopword_r(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_syl(x,y,K):\n",
    "    x['mean_syl'] = x['text'].apply(lambda x: sum(syllable_count(w) for w in x.split())/len(x.split()))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['mean_syl'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of mean number of syllables per word ' + y[:-3])\n",
    "    plt.xlabel('Mean Number of Syllable per Word')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = mean_syl(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch(x,y,K):\n",
    "    x['flesch'] = x['text'].apply(lambda x: flesch_reading_ease(x))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['flesch'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of Flesch reading ease score ' + y[:-3])\n",
    "    plt.xlabel('Flesch Reading Ease Score')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = flesch(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dale(x,y,K):\n",
    "    x['dale'] = x['text'].apply(lambda x: dale_chall_readability_score(x))\n",
    "    plt.figure()\n",
    "    for k in range(K):\n",
    "        xx = x[x[y]==k]\n",
    "        sns.distplot(xx['dale'], hist=True, rug=True, kde_kws={'linewidth': 3}, label=k)\n",
    "    plt.legend(prop={'size': 16}, title = 'Scores')\n",
    "    plt.title('Distribution of Dale-Chall readability score ' + y[:-3])\n",
    "    plt.xlabel('Dale-Chall Readability Score')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdmp = dale(cdmp,'CDMP_R1',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cdmp, xtest_cdmp, ytrain_cdmp, ytest_cdmp = train_test_split(cdmp.iloc[:,1:],cdmp.CDMP_R1, \n",
    "                                                                    test_size=0.3, random_state=1008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cdmp1=xtrain_cdmp.sort_index(axis=0).reset_index(drop=True)\n",
    "ytrain_cdmp1=ytrain_cdmp.sort_index(axis=0).reset_index(drop=True)\n",
    "xtest_cdmp1 = xtest_cdmp.sort_index(axis=0).reset_index(drop=True)\n",
    "ytest_cdmp1 = ytest_cdmp.sort_index(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude glove 200d avg_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Magnitude(r\"glove.6B.200d.magnitude\")\n",
    "\n",
    "def avg_glove(df):\n",
    "    df1 = df.sort_index(axis=0).reset_index(drop=True)\n",
    "    vectors = []\n",
    "    for txt in tqdm_notebook(df1.text.values):\n",
    "        vectors.append(np.average(glove.query(WordPunctTokenizer().tokenize(txt)), axis = 0))\n",
    "    return np.array(vectors)\n",
    "\n",
    "glxtr_cdmp_avg = avg_glove(xtrain_cdmp)\n",
    "glxte_cdmp_avg = avg_glove(xtest_cdmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "dfgltr_cdmp_avg = pd.DataFrame(data = glxtr_cdmp_avg[0:,0:],\n",
    "                              index = [i for i in range(glxtr_cdmp_avg.shape[0])],\n",
    "                              columns = ['glove'+str(i) for i in range(glxtr_cdmp_avg.shape[1])])\n",
    "dfgltr_cdmp_avg0 = pd.concat([xtrain_cdmp1, dfgltr_cdmp_avg.reset_index(drop=True)],axis=1, ignore_index=True)\n",
    "head =  list(xtrain_cdmp1.columns) + list(dfgltr_cdmp_avg.columns)\n",
    "dfgltr_cdmp_avg0.columns = head\n",
    "dfgltr_cdmp_avg0 = dfgltr_cdmp_avg0.iloc[:,3:]\n",
    "dfgltr_cdmp_avg00 = scaler.fit_transform(dfgltr_cdmp_avg0)\n",
    "\n",
    "dfglte_cdmp_avg = pd.DataFrame(data = glxte_cdmp_avg[0:,0:],\n",
    "                              index = [i for i in range(glxte_cdmp_avg.shape[0])],\n",
    "                              columns = ['glove'+str(i) for i in range(glxte_cdmp_avg.shape[1])])\n",
    "dfglte_cdmp_avg0 = pd.concat([xtest_cdmp1, dfglte_cdmp_avg.reset_index(drop=True)],axis=1, ignore_index=True)\n",
    "dfglte_cdmp_avg0.columns = head\n",
    "dfglte_cdmp_avg0 = dfglte_cdmp_avg0.iloc[:,3:]\n",
    "dfglte_cdmp_avg00 = scaler.transform(dfglte_cdmp_avg0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_pt = SVC(random_state=19039)\n",
    "\n",
    "svc_param= {'kernel': ['linear','sigmoid','rbf','poly'],\n",
    "            'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'tol':[0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "svc_gs1 = GridSearchCV(estimator = svm1_pt,\n",
    "                      param_grid = svc_param,\n",
    "                      scoring = make_scorer(kappa),\n",
    "                      n_jobs=-1,\n",
    "                      cv = 5)\n",
    "svc_gs1 = svc_gs1.fit(dfgltr_cdmp_avg00,ytrain_cdmp1)\n",
    "print('CDMP (svm pretrained): ', svc_gs1.best_score_)\n",
    "print('CDMP (svm pretrained): ', svc_gs1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs1_pred = svc_gs1.predict(dfglte_cdmp_avg00)\n",
    "svc_gs1_kappa = kappa(svc_gs1_pred,ytest_cdmp1)\n",
    "print('QWK for cdmp (svm2 pretrained): ', round(svc_gs1_kappa,4))\n",
    "plot_cm(svc_gs1, dfglte_cdmp_avg00,ytest_cdmp1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
